# WebXR RGBD Video Player - PRD & Execution Plan

## Product Requirements Document

### Overview
A WebXR application for viewing RGBD video content (RGB color on the left, depth/disparity map on the right). The application will use a view synthesis shader to generate stereoscopic 3D views in real-time. Users will have dynamic control over focal length, screen distance, and the overall depth effect (`invZmin`).

### Core Features

#### 1. Video Playback
- **Format**: RGBD video (left half: RGB color, right half: depth/disparity map)
- **Display**:
  - **VR Mode**: Two planes (left/right) generated by a view synthesis shader, visible only to the corresponding eyes.
  - **Non-VR Mode**: Anaglyph (red/cyan) 3D display generated by the same shader.
- **Audio**: Video sound playback enabled.
- **Aspect Ratio**: Each synthesized view will maintain the correct aspect ratio, derived from half the width of the source video.

#### 2. Screen Geometry & Depth Control
- **Initial Screen Distance**: 100m (default).
- **Focal Length**: Default 1.0 (equivalent to 36mm lens in 35mm format).
- **Screen Width Calculation**: `screenWidth = focal * screenDistance`.
- **Depth Control (`invZmin`)**: A parameter that controls the total amount of depth, corresponding to the maximum disparity (value of 255 in the disparity map).

#### 3. VR Controller Input
- **Left Controller Stick (Forward/Back)**: Screen distance control (1m to 100m).
- **Right Controller Stick (Forward/Back)**: Focal length control (0.5 to 2.0).
- **TBD Controller Input**: A dedicated control (e.g., trigger, grip, or another axis) for adjusting `invZmin`.

### Technical Specifications

#### View Synthesis Shader
- The core of the application will be a custom GLSL shader.
- **Inputs**:
  - A single video texture containing both RGB and depth data.
  - Uniforms for `focal`, `screenDistance`, `invZmin`, and `ipd`.
- **Process**:
  1. The shader samples the RGB color from the left half of the video texture.
  2. It samples the depth value from the right half.
  3. For the right eye's view, it calculates a horizontal pixel shift (disparity) based on the depth value and the `invZmin` parameter.
  4. It applies this shift to the RGB texture coordinates to synthesize the right eye's view.
  5. The left eye's view is the original, un-shifted RGB image.
- **Output**:
  - In VR mode, it will output the appropriate left or right eye view.
  - In non-VR mode, it will combine the views into a single anaglyph (red/cyan) image.

#### Video Source
- The application will accept an RGBD video file via drag-and-drop or a file selector.

---

## Execution Plan

### Phase 1: Setup & Video Integration
1. Update `index.html` and `index.js` for the new RGBD player.
2. Set up the THREE.js scene, camera, and renderer.
3. Implement video loading for the RGBD format (RGB on left, depth on right).
4. Use `THREE.VideoTexture` to get the video into the GPU.

### Phase 2: View Synthesis Shader Implementation
1. Write the vertex shader (can be a simple pass-through shader).
2. Write the fragment shader with the core view synthesis logic:
   - Uniforms for `invZmin`, `ipd`, etc.
   - Logic to sample RGB and depth from the correct halves of the texture.
   - Calculation for the disparity shift.
   - Synthesize the right eye view by shifting texture coordinates.
   - Implement branching logic to output either a single eye's view (for VR) or a combined anaglyph view (for non-VR).
3. Create a `THREE.ShaderMaterial` with this shader.

### Phase 3: Core Stereo Display
1. **Non-VR Mode**:
   - Create a single `THREE.Mesh` with a `THREE.PlaneGeometry` that fills the screen.
   - Apply the `ShaderMaterial` to this mesh.
   - The shader will be in "anaglyph" mode.
2. **VR Mode**:
   - Create two overlapping planes, one for each eye (using `layers.set(1)` and `layers.set(2)`).
   - Apply the same `ShaderMaterial` to both planes.
   - The shader will be in "stereo" mode, and a uniform will be used to tell the shader which eye it is rendering for (or this can be handled by rendering the scene twice with different uniforms).

### Phase 4: Controller Input & UI
1. Implement VR controller input for `screenDistance` and `focal`.
2. Add a new input for controlling the `invZmin` uniform in the shader.
3. Update the UI (both in-page and in-VR HUD) to display the current value of `invZmin`.
4. Ensure all controls feel intuitive and responsive.

### Phase 5: Screen Sizing & Positioning
1. Position the stereo planes at the `screenDistance` from the viewer.
2. Calculate the dimensions of the planes based on `focal` and `screenDistance` to maintain the correct aspect ratio.
3. Ensure the planes always face the viewer.

### Phase 6: Testing & Polish
1. Test with a sample RGBD video file.
2. Verify that the view synthesis is working correctly in both VR and non-VR modes.
3. Test the `invZmin` control and ensure it correctly adjusts the perceived depth.
4. Check for any visual artifacts, such as seams or incorrect occlusions.
5. Add a debug HUD to display the values of all shader uniforms in real-time.

---

## Open Questions

1. **Depth Map Format**: What is the expected format of the depth map (e.g., 8-bit grayscale, color-encoded)? Is it linear depth, or disparity?
2. **Video Codec**: Are there any recommended video codecs or container formats for RGBD video to ensure good performance?
3. **`invZmin` Control**: What is the most intuitive way to map the `invZmin` control to a VR controller?
4. **Performance**: What are the performance implications of the view synthesis shader, especially at high video resolutions?
5. **Artifacts**: How should we handle potential artifacts from the view synthesis, such as missing information in occluded areas?
6. **Non-VR Controls**: Should there be mouse/keyboard controls for `invZmin` in the non-VR anaglyph mode?
